# ğŸ§  IA Resume Scanner

Projeto para anÃ¡lise de currÃ­culos com IA generativa local (Ollama). Compare perfis com vagas, obtenha um score de compatibilidade e receba feedback inteligente com base em requisitos reais.

![Go](https://img.shields.io/badge/Golang-00ADD8?style=for-the-badge&logo=go&logoColor=white)
![Ollama](https://img.shields.io/badge/Ollama-000000?style=for-the-badge&logo=llama&logoColor=white)
![UniDoc](https://img.shields.io/badge/UniDoc-PDF%2FDOCX-blue?style=for-the-badge)
![Swagger](https://img.shields.io/badge/Swagger-85EA2D?style=for-the-badge&logo=swagger&logoColor=black)
![License](https://img.shields.io/badge/License-MIT-blue?style=for-the-badge)
![Status](https://img.shields.io/badge/status-em%20desenvolvimento-yellow?style=for-the-badge)

---

## ğŸ¯ Objetivo

Criar uma plataforma inteligente para:
- ğŸ“¥ Receber currÃ­culos (PDF ou DOCX)
- ğŸ§  Analisar com IA generativa local (LLM via Ollama)
- ğŸ†š Comparar com os requisitos de uma vaga
- ğŸ¯ Retornar um **score de compatibilidade**
- ğŸ—£ï¸ Fornecer **feedback tÃ©cnico personalizado**

---

## âš™ï¸ Stack Utilizada

| Camada        | Tecnologia                                  |
|---------------|----------------------------------------------|
| **Backend**   | Go (Golang) + Gin                           |
| **IA Local**  | Ollama + modelo `mistral` customizado       |
| **Parser**    | UniPDF / UniOffice (via UniDoc)             |
| **Docs API**  | Swaggo + Swagger UI                         |

---

## â˜ï¸ Requisitos Locais

- Go 1.20 ou superior
- [Ollama](https://ollama.com) instalado localmente
- Modelo LLM criado com prompt customizado (veja abaixo)
- Chave UniDoc vÃ¡lida (para extrair texto de `.pdf` e `.docx`)
- Swag instalado globalmente (para Swagger):
```bash
go install github.com/swaggo/swag/cmd/swag@latest
```

---
  
# ğŸ§  Criar modelo customizado no Ollama

Antes de rodar o projeto, vocÃª deve criar um modelo personalizado com o nome IAResumeScanner usando o mistral como base:

```bash
curl -X POST http://localhost:11434/api/create \
  -H "Content-Type: application/json" \
  -d '{
    "model": "IAResumeScanner",
    "from": "mistral",
    "system": "VocÃª Ã© um assistente de recrutamento especializado. Sua personalidade Ã© muito profissional, precisa, objetiva e cordial. VocÃª tem como responsabilidade analisar detalhadamente currÃ­culos e descriÃ§Ãµes de vagas, identificar habilidades, experiÃªncias, formaÃ§Ãµes e lacunas, e fornecer feedback construtivo para ajudar candidatos a melhorarem seu perfil. VocÃª deve focar em clareza, objetividade e insights Ãºteis para seleÃ§Ã£o e desenvolvimento profissional.",
    "parameters": {
      "temperature": 0.3,
      "top_p": 0.9
    }
  }'
```
---

# ğŸš€ Como Rodar Localmente

## 1. Clone o projeto

```bash
git clone https://github.com/matheushermes/IAResumeScanner.git
cd IAResumeScanner
```
## 2. Instale as dependÃªncias

Antes de iniciar o projeto, certifique-se de instalar todas as dependÃªncias necessÃ¡rias executando o comando abaixo:

```bash
go mod tidy
```

## 3. Configure o ambiente

Copie o arquivo de exemplo .env.example para criar seu prÃ³prio arquivo de configuraÃ§Ã£o .env:
```bash
cp .env.example .env
```

Em seguida, edite o arquivo .env com os valores apropriados para o seu ambiente:
```env
API_PORT=5000
UNIDOC_LICENSE_API_KEY=SUACHAVEAQUI
```

## 4. Inicie o modelo no Ollama

Para iniciar o modelo localmente no Ollama, execute o comando abaixo no terminal:
```bash
ollama run IAResumeScanner
```
Esse comando inicializa o modelo IAResumeScanner previamente configurado no Ollama, permitindo que ele fique disponÃ­vel para receber requisiÃ§Ãµes na sua mÃ¡quina local.

> âš ï¸ Certifique-se de que o Ollama estÃ¡ instalado e que o modelo IAResumeScanner foi criado corretamente antes de executar este comando.

## 5. Rode o servidor

Para iniciar o backend, execute o comando abaixo no terminal:
```bash
go run cmd/main.go
```
O servidor estarÃ¡ disponÃ­vel em:
ğŸ“ http://localhost:5000

---

# ğŸ“‚ Endpoints da API
### â• Upload de currÃ­culo
```http
POST /api/v1/scanner/upload
```
- <b>DescriÃ§Ã£o:</b> Endpoint para envio do arquivo do currÃ­culo.
- <b>Formato do arquivo:</b> `multipart/form-data` contendo o campo `file` com extensÃ£o `.pdf` ou `.docx`.

### ğŸ“Š AnÃ¡lise de Compatibilidade com a Vaga
```http
POST /api/v1/scanner/match
```
- <b>DescriÃ§Ã£o:</b> Recebe a descriÃ§Ã£o completa da vaga e retorna uma avaliaÃ§Ã£o da compatibilidade com o currÃ­culo enviado anteriormente.

<b><i>Corpo da RequisiÃ§Ã£o (JSON):</b></i>
```json
{
  "jobDescription": "DescriÃ§Ã£o completa da vaga"
}
```

<b><i>Resposta esperada:</b></i>
```json
{
  "score": 85,
  "pontos_positivos": ["texto", "texto"],
  "pontos_negativos": ["texto", "texto"],
  "recomendacoes": ["texto", "texto"],
  "feedback_geral": "texto"
}
```
---

# ğŸ“„ DocumentaÃ§Ã£o Swagger
ApÃ³s iniciar o servidor, acesse a documentaÃ§Ã£o interativa da API no navegador:
```http
http://localhost:5000/swagger/index.html
```
---

# ğŸ“Œ ObservaÃ§Ãµes TÃ©cnicas

- A anÃ¡lise utiliza prompt estruturado via MCP (Most Common Prompting), garantindo respostas consistentes e formatadas em JSON.
- O parser aceita arquivos nos formatos `.pdf` e `.docx`, extraindo o conteÃºdo atravÃ©s da biblioteca UniDoc.
- A comunicaÃ§Ã£o com a LLM Ã© feita via chamadas HTTP locais ao serviÃ§o Ollama, com timeout configurado para atÃ© 10 minutos, suportando anÃ¡lises extensas.

# ğŸ“„ LicenÃ§a
Este projeto estÃ¡ licenciado sob a <b>MIT License</b>.

<p align="center"> <strong>ğŸš€ Desenvolvido por Matheus Hermes com foco em IA prÃ¡tica, local e Ã©tica.</strong> </p>